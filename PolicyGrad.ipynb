{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PolicyGrad.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpDXsfgrl7H_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install â€” upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xecHesNPtiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfhmmV3PMr9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5jtRST8xyif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tensorboardX import SummaryWriter "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH5lnb2amE-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "gymlogger.set_level(40) #error only\n",
        "from gym.wrappers import Monitor\n",
        "from itertools import count\n",
        "\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from torch.autograd import Variable\n",
        "\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWFiHEGzmNQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOkXel_OBPjM",
        "colab_type": "text"
      },
      "source": [
        "## Random agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoZ5W7s--AcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make('Pong-v0'))\n",
        "observation = env.reset()\n",
        "new_observation = observation\n",
        "prev_input = None\n",
        "done = False\n",
        "for _ in range(300):\n",
        "    new_observation, reward, done, info = env.step(random.randint(1,3))\n",
        "      \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB83c_fJSEAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6_T7GK-SEvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = \"/content/drive/My Drive/PongPolicyGrad\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ga4-PoBSUG",
        "colab_type": "text"
      },
      "source": [
        "## Policy Gradient model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnhlheMQ-n-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Policy_full_connect(nn.Module):\n",
        "    def __init__(self, n_frames=1):\n",
        "        super(Policy_full_connect, self).__init__()\n",
        "        self.n_frames=n_frames\n",
        "        self.lin1 = nn.Linear(self.n_frames*80*80, 200)\n",
        "        self.lin2 = nn.Linear(200, 3) \n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "        \n",
        "    def forward(self, input):\n",
        "        x = F.relu(self.lin1(input))\n",
        "        output = self.lin2(x)\n",
        "        return F.softmax(output, dim=2)\n",
        "\n",
        "    def select_action(self, state, train=True, return_probs=False):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs = self(Variable(state).to('cuda'))\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample() \n",
        "        if train:\n",
        "          self.saved_log_probs.append(m.log_prob(action)) \n",
        "        if return_probs: return probs, action.data[0]\n",
        "        return action.data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8kaC-XyexM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Policy_conv(nn.Module):\n",
        "    def __init__(self, n_frames=1):\n",
        "        super(Policy_conv, self).__init__()\n",
        "        self.n_frames=n_frames\n",
        "        self.conv1 = nn.Conv2d(self.n_frames, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.head = nn.Linear(1568, 3)\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "        \n",
        "    def forward(self, input):\n",
        "        x = F.relu(self.bn1((self.conv1(input))))\n",
        "        x = F.relu(self.bn2((self.conv2(x))))\n",
        "        x = F.relu(self.bn3((self.conv3(x))))\n",
        "        output =  F.softmax(self.head(x.view(x.size(0), -1)), dim=1) #soft max is not necessary for computing te q value\n",
        "        #print(\"res \",output )\n",
        "        return output\n",
        "\n",
        "    def select_action(self, state, train=True, return_probs=False):\n",
        "        #print(\"state shape\", state.shape)\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs = self(Variable(state).to('cuda'))\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample() \n",
        "        if train:\n",
        "            self.saved_log_probs.append(m.log_prob(action)) \n",
        "        if return_probs: return probs, action.data[0]\n",
        "        return action.data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7sjIsY6-s1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, env, model, optimizer, path='', save_fn='log.pkl', restore_fn='log.pkl'):\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "        self.episode_nb = 0\n",
        "        self.history = []\n",
        "        self.losses = []\n",
        "\n",
        "        self.path = path\n",
        "        self.save_fn = save_fn\n",
        "        self.restore_fn = restore_fn\n",
        "\n",
        "    def preprocess(self, I):\n",
        "        \"\"\" preprocess 210x160x3 into 6400 \"\"\"\n",
        "        I = I[35:195]\n",
        "        I = I[::2, ::2, 0]\n",
        "        I[I == 144] = 0\n",
        "        I[I == 109] = 0\n",
        "        I[I != 0 ] = 1\n",
        "        res = I.astype(np.float)\n",
        "        return res if 'conv' in str(self.model.__class__) else res.ravel()\n",
        "\n",
        "    def restore(self):\n",
        "        restore_path = os.path.join(self.path, self.restore_fn)\n",
        "        if os.path.isfile(restore_path):\n",
        "            print(f\"Load Policy Network parametets from {restore_path}\")\n",
        "            state = torch.load(restore_path)\n",
        "            self.model.load_state_dict(state['state_dict'])\n",
        "            self.optimizer.load_state_dict(state['optimizer'])\n",
        "            self.episode_nb = state['episode']\n",
        "            self.history = state['history']\n",
        "            self.losses = state['losses']\n",
        "        else:\n",
        "            print('There is no checkpoint to restore!')\n",
        "\n",
        "    def save(self):\n",
        "        state = {\n",
        "            'episode': self.episode_nb,\n",
        "            'state_dict': self.model.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'history': self.history,\n",
        "            'losses': self.losses\n",
        "        }\n",
        "        save_path = os.path.join(self.path, self.save_fn)\n",
        "        torch.save(state, save_path)\n",
        "        print(f\"Saved model parametets to {save_path}\")\n",
        "\n",
        "    def update(self, batch_size):\n",
        "        R = 0\n",
        "        loss = []\n",
        "        rewards = []\n",
        "        for r in self.model.rewards[::-1]:\n",
        "            R = r + 0.99 * R\n",
        "            rewards.insert(0, R)\n",
        "\n",
        "        # turn rewards to pytorch tensor and standardize\n",
        "        rewards = torch.Tensor(rewards)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
        "        \n",
        "        for log_prob, reward in zip(self.model.saved_log_probs, rewards):\n",
        "            loss.append(-log_prob * reward)\n",
        "\n",
        "        loss = torch.cat(loss).sum()\n",
        "        loss.backward()\n",
        "        if self.episode_nb % batch_size == 0:\n",
        "                print('ep %d: policy network parameters updating...' % (self.episode_nb))\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "        self.losses.append(loss.item())\n",
        "        # clean rewards and saved_actions\n",
        "        del self.model.rewards[:]\n",
        "        del self.model.saved_log_probs[:]\n",
        "\n",
        "    def train(self, batch_size = 10, save_frequency = 50): \n",
        "        self.restore()  \n",
        "        running_reward = None\n",
        "        reward_sum = 0\n",
        "        while True:\n",
        "            self.episode_nb+=1\n",
        "            state = self.env.reset()\n",
        "            latest_states = []\n",
        "            for t in range(20000):\n",
        "                state = self.preprocess(state)\n",
        "                if len(latest_states)==self.model.n_frames:\n",
        "                    latest_states.pop(0)\n",
        "\n",
        "                while len(latest_states)<self.model.n_frames:\n",
        "                    latest_states.append(state)\n",
        "            \n",
        "                action = self.model.select_action(np.array(latest_states))\n",
        "                action = action + 1\n",
        "                \n",
        "                state, reward, done, _ = self.env.step(action)\n",
        "                reward_sum += reward\n",
        "\n",
        "                self.model.rewards.append(reward)\n",
        "                if done:\n",
        "                    # tracking log\n",
        "                    self.history.append(reward_sum)\n",
        "                    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "                    print('resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward))\n",
        "                    reward_sum = 0\n",
        "                    break\n",
        "            \n",
        "            # use policy gradient update model weights\n",
        "            self.update(batch_size)\n",
        "\n",
        "            # Save model\n",
        "            if self.episode_nb % save_frequency == 0:\n",
        "                print('Saving model ...')\n",
        "                self.save()\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3qScGoT_cl3",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4TkmSqV_G0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN 1-frame model\n",
        "env = gym.make('Pong-v0')\n",
        "policy = Policy_conv(n_frames=1).to('cuda')\n",
        "optimizer = optim.RMSprop(policy.parameters(), lr=1e-4, weight_decay=0.99)\n",
        "trainer = Trainer(env, policy, optimizer, path=data_path)#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1twDjAZ3DOez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN 4-frame model\n",
        "policy2 = Policy_conv(n_frames=4).to('cuda')\n",
        "optimizer2 = optim.RMSprop(policy2.parameters(), lr=1e-4, weight_decay=0.99)\n",
        "trainer2 = Trainer(env, policy2, optimizer2, path=data_path, restore_fn='4frames_conv_net_3100.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhhQEBneGMch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FCN 1-frame model\n",
        "policy3 = Policy_full_connect().to('cuda')\n",
        "optimizer3 = optim.RMSprop(policy3.parameters(), lr=1e-4, weight_decay=0.99)\n",
        "trainer3 = Trainer(env, policy3, optimizer3, path=data_path, restore_fn='full_connect_3020.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu2_ut82aGFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trainer.train(batch_size=5)\n",
        "#trainer2.train(batch_size=5)\n",
        "#trainer3.train(batch_size=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CiAp3aVDjOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.restore()\n",
        "trainer2.restore()\n",
        "trainer3.restore()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHZKi_Cd_Gjj",
        "colab_type": "text"
      },
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdOphoRDdYjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(running_mean(trainer.history[:2950],50), label='CNN 1 frame')\n",
        "plt.plot(running_mean(trainer3.history[:2950],50), label='FCN 1 frame')\n",
        "plt.plot(running_mean(trainer2.history[:2950],50), label='CNN 4 frames')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4gr0S1SH4Od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr_state = trainer.preprocess(state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPabJcArIOC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames=pr_states\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "for i in range(len(frames)):\n",
        "    plt.subplot(1, len(frames), i+1)\n",
        "    plt.imshow(frames[i])\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MlL4cW1IoPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_probs(trainer, states):\n",
        "  pr_states = np.array([trainer.preprocess(s) for s in states])\n",
        "  probs=[]\n",
        "  actions=[]\n",
        "  for i in range(4):\n",
        "    prob, action = trainer.model.select_action(np.expand_dims(pr_states[i],axis=0),train=False, return_probs=True)\n",
        "    probs.append(prob)\n",
        "    actions.append(action)\n",
        "  probs=[prob.cpu().detach().numpy().squeeze() for prob in probs]\n",
        "  return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nwX6asrd5vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probs_cnn = get_probs(trainer, states)\n",
        "probs_fcn = get_probs(trainer3, states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNVbea12gfGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def running_mean(x, N):\n",
        "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
        "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z0fvmRhLQpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib as mpl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9vJUuFGSYd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mpl.style.use('default')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px69E83ceN-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames=pr_states\n",
        "labels=['NOOP','UP','DOWN']\n",
        "x = np.arange(len(labels)) \n",
        "width = 0.35\n",
        "\n",
        "fig = plt.figure(figsize=(10,2.3))\n",
        "for i in range(len(frames)):\n",
        "    plt.subplot(1, len(frames), i+1)\n",
        "    axes = plt.gca()\n",
        "    axes.set_xticklabels([' ','NOOP','UP','DOWN'])\n",
        "    axes.set_ylim([0.,1.])\n",
        "    #plt.imshow(frames[i])\n",
        "    \n",
        "    plt.bar(x - width/2 ,height=probs_cnn[i], width=width, label='CNN 1 frame')\n",
        "    plt.bar(x + width/2 ,height=probs_fcn[i], width=width, label='FCN 1 frame')\n",
        "    \n",
        "    #plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R5pZFOuHyhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize feature maps\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "pr_states= np.array([trainer.preprocess(s) for s in states])\n",
        "\n",
        "trainer.model.conv3.register_forward_hook(get_activation('conv3'))\n",
        "trainer.model.select_action(np.expand_dims(pr_states[0], axis=0), train=False, return_probs=True)\n",
        "\n",
        "act = activation['conv3'].squeeze()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0L_N2za5zuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "act.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKrFyTlE_Kql",
        "colab_type": "text"
      },
      "source": [
        "## Visualize CNN features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukufrKRLGMBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(10,3))\n",
        "plt.subplot(1, 4, 1)\n",
        "axes = plt.gca()\n",
        "axes.set_xlabel('input')\n",
        "plt.imshow(pr_states[0])\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 4, i+2)\n",
        "    axes = plt.gca()\n",
        "    axes.set_xlabel('conv'+str(i+1)) \n",
        "    plt.imshow(activation['conv'+str(i+1)].squeeze().cpu().detach().numpy()[11])\n",
        "    \n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8A8boXHJP4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tb = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UieveR_BDo0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "his_cnn_1 = trainer.history\n",
        "his_cnn_4 = trainer2.history\n",
        "his_fc = trainer3.history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUpUkTdEJV52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(2900):\n",
        "    tb.add_scalars('Reward after episode', {'CNN 1 frame': his_cnn_1[i],\n",
        "                              'CNN 4 frames': his_cnn_4[i],\n",
        "                              'FCN 1 frame': his_fc[i]}, i)\n",
        "    #tb.add_scalar(\"log\", his_cnn_1[i], i)\n",
        "    #tb.add_scalar(\"log\", his_cnn_4[i], i)\n",
        "    #tb.add_scalar(\"log\", his_fc[i], i)\n",
        "tb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAIRnEaZQtew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill 2583"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMh9U6U5J8W1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teFPPb5rEJ3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.savefig()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kggiPC0vK4A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show(threshold, steps=10000):\n",
        "  running_reward=-21\n",
        "  while running_reward<threshold:\n",
        "    obs=[]\n",
        "    running_reward=0\n",
        "    show_env = wrap_env(gym.make('Pong-v0'))\n",
        "    state = show_env.reset()\n",
        "    for _ in range(steps):\n",
        "        obs.append(state)\n",
        "        state = trainer3.preprocess(state)\n",
        "        \n",
        "        action = trainer3.model.select_action(np.expand_dims(state,axis=0),train=False)\n",
        "        action = action + 1\n",
        "        \n",
        "        state, reward, done, _ = show_env.step(action)\n",
        "        running_reward+=reward\n",
        "        if done: break\n",
        "    print(running_reward)\n",
        "\n",
        "  show_env.close()\n",
        "  show_video()\n",
        "  return obs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkkqdChyPnaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obs=show(-20)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}